{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZl7rhtjbISG"
      },
      "source": [
        "# Decision-Making Under Pressure: Evidence from the National Football League\n",
        "\n",
        "## Abstract\n",
        "\n",
        "How does time pressure influence the quality of human decision-making? This study leverages detailed play-by-play data from the 2024 National Football League (NFL) season to model how decision outcomes vary under different levels of temporal constraint, situational stress, and risk. Using machine learning models trained on variables such as time_to_throw, quarter_seconds_remaining, score_differential, down, ydstogo, and was_pressure, we estimate the probability of a \"successful\" play—defined by positive expected points added (EPA) or first-down conversion—conditional on the decision context and available time.\n",
        "\n",
        "Our central research question asks: To what extent does decision quality degrade or improve as available decision time decreases, and under what conditions do individuals or organizations perform optimally under stress? By examining NFL play-calling and execution as a repeated, high-stakes, time-constrained decision environment, we aim to isolate the behavioral and structural determinants of performance under pressure.\n",
        "\n",
        "## Policy Relevance\n",
        "\n",
        "The findings will inform public policy domains where rapid, high-impact decision-making is critical—such as emergency response, military command, public safety, and crisis management. Policymakers often assume that training or hierarchical command can offset stress-induced performance degradation, but evidence on this relationship remains limited and context-dependent.\n",
        "\n",
        "For instance:\n",
        "\n",
        "- Emergency management agencies could use insights from this study to calibrate training scenarios that simulate the optimal level of time stress, improving coordination under pressure.\n",
        "\n",
        "- Law enforcement and defense institutions could refine standard operating procedures to allow decision autonomy or time buffers when reaction time is linked to error rates.\n",
        "\n",
        "- Fiscal policy planners could apply these findings to understand how decision compression near budget deadlines affects accuracy or risk aversion.\n",
        "\n",
        "In practical terms, the study provides a behavioral and data-driven framework for designing systems that preserve decision quality when time is scarce—a persistent governance challenge from disaster response to financial regulation.\n",
        "\n",
        "## Theoretical and Disciplinary Contribution\n",
        "\n",
        "Within the broader social science and behavioral decision-making literature, this research advances the empirical study of bounded rationality and performance under time constraints. Prior work on time pressure often relies on laboratory experiments with limited ecological validity. By contrast, this project introduces a novel, real-world, high-frequency dataset that captures thousands of independent decision episodes—each with quantifiable outcomes, risk levels, and contextual stressors.\n",
        "\n",
        "It contributes to the literature on:\n",
        "\n",
        "- **Behavioral economics:** testing how stress and framing affect risk preferences.\n",
        "\n",
        "- **Organizational behavior:** modeling how teams coordinate information and execute plans under compressed timelines.\n",
        "\n",
        "- **Public administration:** translating insights from competitive sport into the design of resilient, high-performance bureaucracies.\n",
        "\n",
        "In doing so, the project situates sports analytics as a powerful natural laboratory for understanding institutional decision processes at scale.\n",
        "\n",
        "## Why NFL-Level Data Is Uniquely Suited\n",
        "\n",
        "NFL play-by-play data provides a uniquely rich empirical context to study decision-making under pressure:\n",
        "\n",
        "- **High frequency of structured, consequential decisions** — each play represents a controlled decision with clear inputs, timing, and outcomes.\n",
        "\n",
        "- **Quantifiable stress indicators** — the data contain proxies for time pressure (quarter_seconds_remaining, score_differential, down), situational difficulty (ydstogo, defense_personnel), and physical/psychological stressors (was_pressure, temperature, crowd noise where available).\n",
        "\n",
        "- **Repeated trials under consistent rules** — thousands of similar decisions allow for causal inference and model training across standardized conditions.\n",
        "\n",
        "- **Observable outcomes** — each decision has an objective result (EPA, success/failure), enabling direct measurement of decision quality.\n",
        "\n",
        "- **Human and organizational parallels** — NFL coaching and player decision-making occur within complex hierarchies, teamwork constraints, and real-time communication systems analogous to public institutions under stress.\n",
        "\n",
        "Thus, professional football offers a rare \"natural experiment\" for modeling decision-making dynamics that are otherwise difficult to observe in real policy settings—where stakes are high, conditions are dynamic, and data are scarce.\n",
        "\n",
        "## In Summary\n",
        "\n",
        "This project bridges sports analytics and behavioral public policy, demonstrating how machine learning applied to NFL decision data can generate actionable insights for improving human and institutional performance under stress. By quantifying the relationship between time pressure and decision quality, the study contributes both to academic theory and to the practical design of systems that safeguard judgment in moments when it matters most."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL3Gp5B1aWJz"
      },
      "source": [
        "# Predictor Variables (Features)\n",
        "\n",
        "We can categorize predictors into five conceptual groups — contextual, temporal, risk-based, environmental, and human factors.\n",
        "\n",
        "## A. Temporal / Pressure Features\n",
        "\n",
        "Capture how much time or situational stress was present:\n",
        "\n",
        "- `quarter_seconds_remaining`\n",
        "- `half_seconds_remaining`\n",
        "- `game_seconds_remaining`\n",
        "- `score_differential`\n",
        "- `quarter` (proxy for rising time pressure)\n",
        "- `timeout_team` or number of `timeouts_remaining`\n",
        "- `time_to_throw` (for QB decisions)\n",
        "- `play_clock` (if available)\n",
        "- `crowd_noise` (can be appended from stadium / attendance data)\n",
        "\n",
        "**Interpretation:** Analogous to how decision-making in crisis situations worsens as time pressure increases.\n",
        "\n",
        "## B. Situational Context Features\n",
        "\n",
        "Reflect the strategic environment or difficulty:\n",
        "\n",
        "- `down`\n",
        "- `ydstogo`\n",
        "- `yardline_100` (distance from end zone)\n",
        "- `goal_to_go`\n",
        "- `posteam` vs. `defteam` strength indicators (can be appended from team EPA averages)\n",
        "- `score_differential_post`\n",
        "\n",
        "**Interpretation:** How situational constraints interact with decision pressure.\n",
        "\n",
        "## C. Decision Characteristics\n",
        "\n",
        "What kind of action was chosen:\n",
        "\n",
        "- `play_type` (categorical: pass, run, field_goal, etc.)\n",
        "- `qb_dropback`, `shotgun`, `no_huddle`\n",
        "- `pass_length`, `pass_location`, `run_gap`, `run_location`\n",
        "- `offense_personnel` (number of RBs, TEs, WRs on field)\n",
        "- `defense_personnel` or `number_of_pass_rushers`\n",
        "\n",
        "**Interpretation:** The structure of the decision — strategy under stress.\n",
        "\n",
        "## D. Risk and Aggression Indicators\n",
        "\n",
        "Proxies for risk appetite:\n",
        "\n",
        "- `air_yards` (longer passes = riskier)\n",
        "- `fg_prob`, `td_prob`, `no_score_prob`\n",
        "- `fourth_down_converted` / `fourth_down_failed` (rare risk decisions)\n",
        "- `xpass` and `pass_oe` (model-based pass probability and over-expectation)\n",
        "\n",
        "**Interpretation:** How risk preferences evolve under pressure (risk aversion vs. overconfidence).\n",
        "\n",
        "## E. Environmental / Exogenous Variables\n",
        "\n",
        "Contextual conditions outside decision-maker control:\n",
        "\n",
        "- `temp`, `wind`, `roof`, `surface`, `weather`\n",
        "- `home_team` / `away_team` (home-field stress factor)\n",
        "- `stadium` or `stadium_id` (for fixed effects)\n",
        "- `week` (season progression, fatigue)\n",
        "\n",
        "**Interpretation:** How external conditions moderate performance under stress.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG7w_FMHbW24"
      },
      "source": [
        "# What the Predictors Represent in Football Terms\n",
        "\n",
        "The model's predictors might look football-specific, but conceptually they map cleanly onto universal features of human decision-making:\n",
        "| Variable | Football Meaning | Decision Science Analogue | Policy Analogue |\n",
        "|----------|------------------|---------------------------|-----------------|\n",
        "| `time_to_throw` | Time (in seconds)<br>QB takes before<br>making a decision | Decision latency:<br>time pressure vs<br>deliberation | Time available before<br>making a judgment —<br>e.g. emergency responder<br>deciding whether to evacuate |\n",
        "| `quarter_seconds_remaining` | Time remaining<br>in game/half | Temporal scarcity | How deadline proximity<br>affects risk tolerance<br>(budget cycles,<br>crisis escalation) |\n",
        "| `score_differential` | Margin between<br>teams | Performance feedback /<br>situational stress | How perceived success<br>or failure affects risk<br>behavior (fiscal surpluses<br>vs deficits) |\n",
        "| `down` and `ydstogo` | Options and<br>constraints on<br>next decision | Constraint<br>complexity | How limited options<br>(few resources, limited<br>tools) shape policy<br>decisions |\n",
        "| `was_pressure` | Binary indicator<br>of defensive<br>pressure | Acute stress<br>event | External shocks<br>(market crash, natural<br>disaster, hostile event)<br>that reduce cognitive<br>bandwidth |\n",
        "| `pass_length`, `air_yards` | Aggressiveness<br>of decision | Risk appetite | How decision-makers<br>adjust between conservative<br>vs bold actions under<br>uncertainty |\n",
        "| `home_away` | Environment | Familiarity /<br>environmental<br>comfort | Bureaucrats working<br>in home vs foreign<br>context; institutional<br>familiarity |\n",
        "| `temperature`, `wind`, `roof` | Physical<br>conditions | Environmental<br>stressors | Fatigue, heat, or<br>information overload<br>that degrades<br>performance |\n",
        "\n",
        "These predictors — while drawn from the NFL — are really structured proxies for the general dimensions that drive human performance under pressure: time pressure, environmental stress, feedback loops, resource constraints, and risk-taking vs. conservatism.\n",
        "\n",
        "---\n",
        "\n",
        "# What Policymakers Could Learn from the Model\n",
        "\n",
        "The insight isn't how football players behave, but how humans make complex, time-bounded, feedback-driven decisions when stakes are high.\n",
        "\n",
        "## A. Understanding the \"decision-speed vs. quality\" trade-off\n",
        "\n",
        "If the model finds that shorter `time_to_throw` correlates with lower EPA/WPA only under certain stress conditions (e.g., trailing by a small margin), it mirrors findings in crisis response: faster isn't always better — speed improves outcomes up to a point, then sharply degrades quality.\n",
        "\n",
        "**Policy takeaway:**\n",
        "- In emergency operations, allow structured seconds for deliberation when possible.\n",
        "- Overly rigid time targets (e.g., \"decide within 30 seconds\") can reduce optimal outcomes.\n",
        "\n",
        "## B. Stress and situational awareness degradation\n",
        "\n",
        "The variable `was_pressure` quantifies plays made under acute stress. If the model shows significantly worse decision outcomes when `was_pressure = 1`, even controlling for time and score, it suggests stress distinctly erodes performance, beyond just time pressure.\n",
        "\n",
        "**Policy takeaway:**\n",
        "- Crisis training should separate stress resilience from decision timing — they are not interchangeable.\n",
        "- For example, FEMA or military training could prioritize realistic stress inoculation over purely procedural drills.\n",
        "\n",
        "## C. Feedback loops and overcorrection\n",
        "\n",
        "When behind (`score_differential < 0`), QBs often take more risks (`pass_length` ↑), but the model could show that these don't necessarily improve outcomes (`wpa` ↓).\n",
        "\n",
        "**Policy analogy:**\n",
        "- Leaders facing \"deficits\" (budgetary, political, or reputational) may overcorrect by making riskier policy bets that statistically worsen long-term outcomes.\n",
        "- Encourages institutional design that buffers against overreaction to short-term losses.\n",
        "\n",
        "## D. Adaptive risk calibration\n",
        "\n",
        "Some QBs consistently make better decisions (higher `epa`/`wpa`) under similar conditions — this measures expert adaptability.\n",
        "\n",
        "**Policy takeaway:**\n",
        "- Identify and train for \"decision composure profiles\" — people who maintain consistent judgment under stress should be cultivated for roles like crisis negotiators, emergency managers, and diplomats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "FShsoC5KT4W5"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade --force-reinstall nfl_data_py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-OV94kanZYg"
      },
      "source": [
        "# Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ffd06ce",
        "outputId": "bb01bbc6-616c-486d-eec0-877a3872eb32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024 done.\n",
            "2023 done.\n",
            "2022 done.\n",
            "Downcasting floats.\n",
            "Successfully loaded data for 2024, 2023, and 2022.\n"
          ]
        }
      ],
      "source": [
        "import nfl_data_py as nfl\n",
        "import pandas as pd\n",
        "\n",
        "# # Load the NFL play-by-play data for the specified years\n",
        "df = nfl.import_pbp_data([2024, 2023, 2022]) # Load data for 2024, 2023, and 2022\n",
        "print(\"Successfully loaded data for 2024, 2023, and 2022.\")\n",
        "\n",
        "# df.to_csv('data.csv')\n",
        "# df = pd.read_csv('/content/data.csv')\n",
        "# df['season'].value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1Ue9N91gxcH"
      },
      "source": [
        "**Create \"Thus Far\" Indicators for each game**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "tEvRK9AMg7SF"
      },
      "outputs": [],
      "source": [
        "EVENTS_FOR_OFFENSE = [\n",
        "    # special teams – negative/rare events for the punting/kicking offense\n",
        "    \"punt_blocked\",\n",
        "\n",
        "    # downs/outcomes (offense-oriented)\n",
        "    \"first_down_rush\",\"first_down_pass\",\"first_down_penalty\",\n",
        "    \"third_down_converted\",\"third_down_failed\",\n",
        "    \"fourth_down_converted\",\"fourth_down_failed\",\n",
        "\n",
        "    # pass outcomes\n",
        "    \"incomplete_pass\",\n",
        "\n",
        "    # kick/punt results credited to the kicking team (posteam)\n",
        "    \"touchback\",              # we’ll scope it to punts/kickoffs for the kicking team\n",
        "    \"punt_inside_twenty\",\"punt_in_endzone\",\"punt_out_of_bounds\",\"punt_downed\",\"punt_fair_catch\",\n",
        "\n",
        "    # ball security / turnovers (against the offense)\n",
        "    \"interception\",\n",
        "    \"fumble_forced\",\"fumble_not_forced\",\"fumble_out_of_bounds\",\"fumble_lost\",\"fumble\",\n",
        "\n",
        "    # defensive production against the offense (on offense snaps)\n",
        "    \"solo_tackle\",\"assist_tackle\",\"tackled_for_loss\",\"qb_hit\",\"sack\",\n",
        "\n",
        "    # penalties (offense only)\n",
        "    \"penalty\",\n",
        "\n",
        "    # scoring / conversions (offense)\n",
        "    \"touchdown\",\"pass_touchdown\",\"rush_touchdown\",\"return_touchdown\",\n",
        "    \"extra_point_attempt\",\"two_point_attempt\",\"field_goal_attempt\",\n",
        "\n",
        "    # special teams usage (offense as kicking team)\n",
        "    \"kickoff_attempt\",\"punt_attempt\",\n",
        "\n",
        "    # usage (offense snaps)\n",
        "    \"rush_attempt\",\"pass_attempt\",\"complete_pass\",\n",
        "\n",
        "    # laterals (offense context)\n",
        "    \"lateral_reception\",\"lateral_rush\",\n",
        "\n",
        "    # safety against offense\n",
        "    \"safety\",\n",
        "]\n",
        "\n",
        "# numeric “running totals” (sum) attributed to the offense\n",
        "NUMERIC_FOR_OFFENSE = [\n",
        "    \"return_yards\",   # only when return_team == posteam\n",
        "    \"penalty_yards\",  # only when penalty_team == posteam\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "X3xnTYt2h7_z"
      },
      "outputs": [],
      "source": [
        "def add_offense_so_far(\n",
        "    df: pd.DataFrame,\n",
        "    event_cols = EVENTS_FOR_OFFENSE,\n",
        "    numeric_cols = NUMERIC_FOR_OFFENSE,\n",
        "    include_current: bool = False,\n",
        "    exclude_kneels_spikes_from_usage: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Add cumulative '..._so_far' columns for the possessing offense (posteam) within each game.\n",
        "    Context-aware attribution rules as discussed.\n",
        "    \"\"\"\n",
        "    out = df.copy()\n",
        "\n",
        "    # ---- helpers (float, never int) ----\n",
        "    def _ind(name: str) -> pd.Series:\n",
        "        if name in out.columns:\n",
        "            # coerce weird types -> numeric 0/1; treat missing as 0\n",
        "            return pd.to_numeric(out[name], errors=\"coerce\").fillna(0.0).astype(float)\n",
        "        return pd.Series(0.0, index=out.index)\n",
        "\n",
        "    def _bool(name: str) -> pd.Series:\n",
        "        return (out.get(name, 0)).fillna(0).astype(bool)\n",
        "\n",
        "    def _str(name: str) -> pd.Series:\n",
        "        return out.get(name, \"\").astype(\"string\")\n",
        "\n",
        "    def _num(name: str) -> pd.Series:\n",
        "        return pd.to_numeric(out.get(name, 0), errors=\"coerce\").fillna(0.0).astype(float)\n",
        "\n",
        "    def _eq_team(series_name: str, team_series: pd.Series) -> pd.Series:\n",
        "        if series_name not in out.columns:\n",
        "            return pd.Series(False, index=out.index)\n",
        "        return _str(series_name) == team_series.fillna(\"\")\n",
        "\n",
        "    def _any_eq_team(cols: list[str], team_series: pd.Series) -> pd.Series:\n",
        "        m = pd.Series(False, index=out.index)\n",
        "        for c in cols:\n",
        "            if c in out.columns:\n",
        "                m |= (_str(c) == team_series.fillna(\"\"))\n",
        "        return m\n",
        "\n",
        "    # ---- ordering (for deterministic output) ----\n",
        "    if \"order_sequence\" in out.columns:\n",
        "        out = out.sort_values([\"game_id\",\"posteam\",\"order_sequence\"])\n",
        "    else:\n",
        "        out = out.sort_values([\"game_id\",\"posteam\",\"game_seconds_remaining\",\"play_id\"],\n",
        "                              ascending=[True, True, False, True])\n",
        "\n",
        "    posteam = _str(\"posteam\")\n",
        "    defteam = _str(\"defteam\")\n",
        "\n",
        "    play_type = _str(\"play_type\").str.lower()\n",
        "    is_run = play_type.eq(\"run\")\n",
        "    is_pass = play_type.eq(\"pass\")\n",
        "    is_punt = play_type.eq(\"punt\") | (_ind(\"punt_attempt\").gt(0))\n",
        "    is_kickoff = play_type.eq(\"kickoff\") | (_ind(\"kickoff_attempt\").gt(0))\n",
        "\n",
        "    return_team_is_off = _eq_team(\"return_team\", posteam)\n",
        "    on_offense_scrimmage = is_run | is_pass\n",
        "\n",
        "    valid_usage = pd.Series(True, index=out.index)\n",
        "    if exclude_kneels_spikes_from_usage:\n",
        "        valid_usage &= (~_bool(\"qb_kneel\")) & (~_bool(\"qb_spike\"))\n",
        "\n",
        "    # ---- offense-attributed indicators (float 0/1) ----\n",
        "    off = {}\n",
        "\n",
        "    off[\"punt_blocked\"] = (_ind(\"punt_blocked\") * ((is_punt | is_kickoff).astype(float)))\n",
        "\n",
        "    for c in [\"first_down_rush\",\"first_down_pass\",\"first_down_penalty\",\n",
        "              \"third_down_converted\",\"third_down_failed\",\n",
        "              \"fourth_down_converted\",\"fourth_down_failed\",\n",
        "              \"incomplete_pass\"]:\n",
        "        if c in event_cols:\n",
        "            off[c] = _ind(c)\n",
        "\n",
        "    if \"touchback\" in event_cols:\n",
        "        off[\"touchback\"] = _ind(\"touchback\") * ((is_punt | is_kickoff).astype(float))\n",
        "    for c in [\"punt_inside_twenty\",\"punt_in_endzone\",\"punt_out_of_bounds\",\"punt_downed\",\"punt_fair_catch\"]:\n",
        "        if c in event_cols:\n",
        "            off[c] = _ind(c) * (is_punt.astype(float))\n",
        "\n",
        "    off[\"interception\"] = _ind(\"interception\")\n",
        "\n",
        "    fumbled_by_off = _any_eq_team([c for c in [\"fumbled_1_team\",\"fumbled_2_team\"] if c in out.columns], posteam)\n",
        "    off[\"fumble\"] = _ind(\"fumble\") * (fumbled_by_off.astype(float))\n",
        "    off[\"fumble_lost\"] = (\n",
        "        _ind(\"fumble_lost\")\n",
        "        * (fumbled_by_off & _any_eq_team(\n",
        "            [c for c in [\"fumble_recovery_1_team\",\"fumble_recovery_2_team\"] if c in out.columns], defteam\n",
        "          )).astype(float)\n",
        "    )\n",
        "    forced_by_def = _any_eq_team([c for c in [\"forced_fumble_player_1_team\",\"forced_fumble_player_2_team\"] if c in out.columns], defteam)\n",
        "    off[\"fumble_forced\"] = _ind(\"fumble_forced\") * ((forced_by_def & (on_offense_scrimmage | return_team_is_off)).astype(float))\n",
        "    off[\"fumble_not_forced\"] = _ind(\"fumble_not_forced\") * (fumbled_by_off.astype(float))\n",
        "    off[\"fumble_out_of_bounds\"] = _ind(\"fumble_out_of_bounds\") * (fumbled_by_off.astype(float))\n",
        "\n",
        "    solo_def = _any_eq_team([c for c in [\"solo_tackle_1_team\",\"solo_tackle_2_team\"] if c in out.columns], defteam)\n",
        "    assist_def = _any_eq_team(\n",
        "        [c for c in [\"assist_tackle_1_team\",\"assist_tackle_2_team\",\"assist_tackle_3_team\",\"assist_tackle_4_team\"] if c in out.columns],\n",
        "        defteam,\n",
        "    )\n",
        "    off[\"solo_tackle\"]   = (_ind(\"solo_tackle\").gt(0) & solo_def & (on_offense_scrimmage | return_team_is_off)).astype(float)\n",
        "    off[\"assist_tackle\"] = (_ind(\"assist_tackle\").gt(0) & assist_def & (on_offense_scrimmage | return_team_is_off)).astype(float)\n",
        "    off[\"tackled_for_loss\"] = _ind(\"tackled_for_loss\") * (on_offense_scrimmage.astype(float))\n",
        "    off[\"qb_hit\"] = _ind(\"qb_hit\") * (on_offense_scrimmage.astype(float))\n",
        "    off[\"sack\"]   = _ind(\"sack\") * (on_offense_scrimmage.astype(float))\n",
        "\n",
        "    off[\"penalty\"] = _ind(\"penalty\") * (_eq_team(\"penalty_team\", posteam).astype(float))\n",
        "\n",
        "    for c in [\"touchdown\",\"pass_touchdown\",\"rush_touchdown\",\"return_touchdown\",\n",
        "              \"extra_point_attempt\",\"two_point_attempt\",\"field_goal_attempt\"]:\n",
        "        if c in event_cols:\n",
        "            if c == \"return_touchdown\":\n",
        "                off[c] = _ind(c) * (_eq_team(\"return_team\", posteam).astype(float))\n",
        "            elif c == \"touchdown\":\n",
        "                off[c] = _ind(c) * (\n",
        "                    (_eq_team(\"td_team\", posteam) | _ind(\"pass_touchdown\").gt(0) | _ind(\"rush_touchdown\").gt(0)).astype(float)\n",
        "                )\n",
        "            else:\n",
        "                off[c] = _ind(c)\n",
        "\n",
        "    off[\"kickoff_attempt\"] = _ind(\"kickoff_attempt\")\n",
        "    off[\"punt_attempt\"]    = _ind(\"punt_attempt\")\n",
        "\n",
        "    off[\"rush_attempt\"] = _ind(\"rush_attempt\") * (valid_usage.astype(float))\n",
        "    off[\"pass_attempt\"] = _ind(\"pass_attempt\") * (valid_usage.astype(float))\n",
        "    off[\"complete_pass\"] = _ind(\"complete_pass\")\n",
        "\n",
        "    off[\"lateral_reception\"] = _ind(\"lateral_reception\") * (is_pass.astype(float))\n",
        "    off[\"lateral_rush\"]      = _ind(\"lateral_rush\") * (is_run.astype(float))\n",
        "\n",
        "    off[\"safety\"] = _ind(\"safety\")\n",
        "\n",
        "    # ---- numeric running totals (float) ----\n",
        "    num = {}\n",
        "    if \"return_yards\" in numeric_cols:\n",
        "        num[\"return_yards\"] = _num(\"return_yards\").where(return_team_is_off, 0.0)\n",
        "    if \"penalty_yards\" in numeric_cols:\n",
        "        num[\"penalty_yards\"] = _num(\"penalty_yards\").where(_eq_team(\"penalty_team\", posteam), 0.0)\n",
        "\n",
        "    # ---- cumulative per (game_id, posteam) without integer casting ----\n",
        "    so_far_cols = []\n",
        "\n",
        "    # helper: groupby cumsum for a Series (not necessarily in out)\n",
        "    def gb_cumsum(series: pd.Series) -> pd.Series:\n",
        "        return series.groupby([out[\"game_id\"], out[\"posteam\"]]).cumsum()\n",
        "\n",
        "    # events\n",
        "    for c in event_cols:\n",
        "        ser = off.get(c, _ind(c))  # offense-attributed if we built it; else raw indicator\n",
        "        ser = ser.fillna(0.0)\n",
        "        cum = gb_cumsum(ser)\n",
        "        if not include_current:\n",
        "            cum = cum - ser\n",
        "        out[f\"{c}_so_far\"] = cum.fillna(0.0).astype(float)\n",
        "        so_far_cols.append(f\"{c}_so_far\")\n",
        "\n",
        "    # numeric sums\n",
        "    for c, ser in num.items():\n",
        "        ser = ser.fillna(0.0)\n",
        "        cum = gb_cumsum(ser)\n",
        "        if not include_current:\n",
        "            cum = cum - ser\n",
        "        out[f\"{c}_so_far\"] = cum.fillna(0.0).astype(float)\n",
        "        so_far_cols.append(f\"{c}_so_far\")\n",
        "\n",
        "    # print the list so you can paste into your feature set\n",
        "    print(\"New cumulative columns created:\")\n",
        "    print(so_far_cols)\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7uWRkj9mUjM",
        "outputId": "1ba6b769-9ae1-4687-a174-dd80691cac94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New cumulative columns created:\n",
            "['punt_blocked_so_far', 'first_down_rush_so_far', 'first_down_pass_so_far', 'first_down_penalty_so_far', 'third_down_converted_so_far', 'third_down_failed_so_far', 'fourth_down_converted_so_far', 'fourth_down_failed_so_far', 'incomplete_pass_so_far', 'touchback_so_far', 'punt_inside_twenty_so_far', 'punt_in_endzone_so_far', 'punt_out_of_bounds_so_far', 'punt_downed_so_far', 'punt_fair_catch_so_far', 'interception_so_far', 'fumble_forced_so_far', 'fumble_not_forced_so_far', 'fumble_out_of_bounds_so_far', 'fumble_lost_so_far', 'fumble_so_far', 'solo_tackle_so_far', 'assist_tackle_so_far', 'tackled_for_loss_so_far', 'qb_hit_so_far', 'sack_so_far', 'penalty_so_far', 'touchdown_so_far', 'pass_touchdown_so_far', 'rush_touchdown_so_far', 'return_touchdown_so_far', 'extra_point_attempt_so_far', 'two_point_attempt_so_far', 'field_goal_attempt_so_far', 'kickoff_attempt_so_far', 'punt_attempt_so_far', 'rush_attempt_so_far', 'pass_attempt_so_far', 'complete_pass_so_far', 'lateral_reception_so_far', 'lateral_rush_so_far', 'safety_so_far', 'return_yards_so_far', 'penalty_yards_so_far']\n"
          ]
        }
      ],
      "source": [
        "df = add_offense_so_far(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "8GolJY7VvD0R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "df[\"completion_rate_so_far\"] = df[\"complete_pass_so_far\"] / df[\"pass_attempt_so_far\"].replace(0, np.nan)\n",
        "df[\"pass_rate_so_far\"] = df[\"pass_attempt_so_far\"] / (df[\"rush_attempt_so_far\"] + df[\"pass_attempt_so_far\"]).replace(0, np.nan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2EHiMpdhBQZ"
      },
      "source": [
        "**Create \"Last Drive\" Indicators**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "TiGstYW3hDtb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1) Last play of each (game, team, drive)\n",
        "last_idx = (\n",
        "    df.dropna(subset=[\"posteam\"])\n",
        "      .groupby([\"game_id\",\"posteam\",\"drive\"])[\"order_sequence\"]\n",
        "      .idxmax()\n",
        ")\n",
        "ends = df.loc[last_idx, [\"game_id\",\"posteam\",\"drive\",\"fixed_drive_result\"]].copy()\n",
        "v = ends[\"fixed_drive_result\"].fillna(\"\").str.lower()\n",
        "\n",
        "# 2) Drive-ending flags for that drive\n",
        "ends[\"drive_td\"]   = v.str.contains(\"touchdown\").astype(int)\n",
        "ends[\"drive_fg\"]   = v.str.contains(\"field goal\").astype(int)\n",
        "ends[\"drive_punt\"] = v.str.contains(\"punt\").astype(int)\n",
        "ends[\"drive_tod\"]  = v.str.contains(\"downs\").astype(int)     # turnover on downs\n",
        "\n",
        "# 3) Team-relative drive number (1,2,3,…) using earliest play in that drive\n",
        "starts = (\n",
        "    df.dropna(subset=[\"posteam\"])\n",
        "      .groupby([\"game_id\",\"posteam\",\"drive\"])[\"order_sequence\"]\n",
        "      .min()\n",
        "      .reset_index(name=\"drive_start_order\")\n",
        "      .sort_values([\"game_id\",\"posteam\",\"drive_start_order\"])\n",
        ")\n",
        "starts[\"team_drive_number\"] = starts.groupby([\"game_id\",\"posteam\"]).cumcount() + 1\n",
        "\n",
        "# attach team_drive_number to drive ends\n",
        "ends = ends.merge(starts, on=[\"game_id\",\"posteam\",\"drive\"], how=\"left\")\n",
        "\n",
        "# 4) Previous-drive flags → shift forward one drive number\n",
        "prev = (\n",
        "    ends[[\"game_id\",\"posteam\",\"team_drive_number\",\"drive_td\",\"drive_fg\",\"drive_punt\",\"drive_tod\"]]\n",
        "      .rename(columns={\n",
        "          \"drive_td\":\"prev_drive_td\", \"drive_fg\":\"prev_drive_fg\",\n",
        "          \"drive_punt\":\"prev_drive_punt\", \"drive_tod\":\"prev_drive_tod\"\n",
        "      })\n",
        ")\n",
        "prev[\"team_drive_number\"] += 1  # these belong to the *next* drive\n",
        "\n",
        "# 5) Put team_drive_number on every play row, then merge prev-drive flags\n",
        "df = df.merge(\n",
        "    starts[[\"game_id\",\"posteam\",\"drive\",\"team_drive_number\"]],\n",
        "    on=[\"game_id\",\"posteam\",\"drive\"], how=\"left\"\n",
        ").merge(\n",
        "    prev, on=[\"game_id\",\"posteam\",\"team_drive_number\"], how=\"left\"\n",
        ")\n",
        "\n",
        "# 6) First drive has no previous drive → zeros\n",
        "df[[\"prev_drive_td\",\"prev_drive_fg\",\"prev_drive_punt\",\"prev_drive_tod\"]] = \\\n",
        "    df[[\"prev_drive_td\",\"prev_drive_fg\",\"prev_drive_punt\",\"prev_drive_tod\"]].fillna(0).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOTtxxTOgKIR"
      },
      "source": [
        "**Calculate Avg Passing Yard Statistics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "6TcY9H4HGZ_L"
      },
      "outputs": [],
      "source": [
        "f = (\n",
        "    (df[\"play_type\"] == \"pass\")\n",
        "    & (df.get(\"play_deleted\", 0) == 0)\n",
        "    & (df.get(\"aborted_play\", 0) == 0)\n",
        ")\n",
        "cols = [\"season\", \"defteam\", \"game_id\", \"passing_yards\", \"sack\", \"yards_gained\"]\n",
        "p = df.loc[f, cols].copy()\n",
        "\n",
        "per_game = (\n",
        "    p.groupby([\"season\", \"defteam\", \"game_id\"])[\"passing_yards\"]\n",
        "     .sum(min_count=1)              # sums completed-pass yards; NaNs ignored\n",
        "     .fillna(0)                     # all-incomplete games become 0\n",
        "     .reset_index(name=\"pass_yards_allowed\")\n",
        ")\n",
        "avg_pass_allowed = (\n",
        "    per_game.groupby([\"season\", \"defteam\"])[\"pass_yards_allowed\"]\n",
        "            .mean()\n",
        "            .reset_index(name=\"avg_pass_yards_allowed_per_game\")\n",
        ")\n",
        "# Filter to offensive pass plays (ignore deleted/aborted)\n",
        "mask = (\n",
        "    (df[\"play_type\"] == \"pass\")\n",
        "    & (df.get(\"play_deleted\", 0) == 0)\n",
        "    & (df.get(\"aborted_play\", 0) == 0)\n",
        ")\n",
        "\n",
        "# Keep only relevant columns\n",
        "cols = [\"season\", \"posteam\", \"game_id\", \"passing_yards\"]\n",
        "p = df.loc[mask, cols].copy()\n",
        "\n",
        "# Sum passing yards per game for each offense\n",
        "per_game = (\n",
        "    p.groupby([\"season\", \"posteam\", \"game_id\"])[\"passing_yards\"]\n",
        "     .sum(min_count=1)\n",
        "     .fillna(0)\n",
        "     .reset_index(name=\"pass_yards_gained\")\n",
        ")\n",
        "\n",
        "# Average per team, per season\n",
        "avg_pass_gained = (\n",
        "    per_game.groupby([\"season\", \"posteam\"])[\"pass_yards_gained\"]\n",
        "            .mean()\n",
        "            .reset_index(name=\"avg_pass_yards_gained_per_game\")\n",
        ")\n",
        "df = df.merge(\n",
        "    avg_pass_gained,\n",
        "    how=\"left\",\n",
        "    left_on=[\"season\", \"posteam\"],\n",
        "    right_on=[\"season\", \"posteam\"]\n",
        ").merge(\n",
        "    avg_pass_allowed,\n",
        "    how=\"left\",\n",
        "    left_on=[\"season\", \"defteam\"],\n",
        "    right_on=[\"season\", \"defteam\"]\n",
        ").rename(\n",
        "    columns={\n",
        "        \"avg_pass_yards_gained_per_game\": \"team_avg_pass_yards_gained_per_game\",\n",
        "        \"avg_pass_yards_allowed_per_game\": \"team_avg_pass_yards_allowed_per_game\"\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfhbAaxRf51h"
      },
      "source": [
        "**Create Continuous Sucess Measure**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "KktCl6u3vXtf"
      },
      "outputs": [],
      "source": [
        "# # Replace 0 in 'ydstogo' with a small number to avoid division by zero\n",
        "# df['sucess_outcome'] = df['yards_gained'] / df['yardline_100']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DucSUv4XoML6"
      },
      "source": [
        "Bounded & interpretable: 0 = worst (turnover, safety, or no forward progress), 1 = TD or full distance to the goal, everything else is “% of field to go that you gained.”\n",
        "\n",
        "Handles nasty cases: caps long gains at 1, floors losses at 0, protects against yardline_100 == 0, and doesn’t punish intentional spikes/kneels (NaN by default).\n",
        "\n",
        "QB-relevant overrides: INT / fumble lost / 4th-down fail / safety are hard 0’s; offensive penalties are 0; defensive penalties count as positive yardage.\n",
        "\n",
        "Non-scrimmage is out of scope by default (you can include them if you want, but the interpretation changes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "FGjvtnRmn7YX"
      },
      "outputs": [],
      "source": [
        "def compute_success_outcome(\n",
        "    df: pd.DataFrame,\n",
        "    drop_non_scrimmage: bool = True,\n",
        "    treat_spike_kneel_as_na: bool = True,\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    0..1 success for the possessing offense on this snap.\n",
        "\n",
        "    Base: share of field-to-go gained on the snap: max(yards_gained, 0) / max(yardline_100, 1)\n",
        "      • Caps at 1 (TD-level gain), floors at 0 (no forward progress or loss).\n",
        "    Overrides (QB-intuitive):\n",
        "      • Offensive TD (pass/rush/td_team==posteam) -> 1.0\n",
        "      • Turnover against offense (interception OR fumble_lost by posteam OR 4th_down_failed OR safety) -> 0.0\n",
        "      • Offensive penalty (accepted) -> 0.0\n",
        "      • Defensive penalty (accepted) -> use max(gain, penalty_yards) for the base share\n",
        "      • Spikes/Kneels -> NaN (don’t teach the model they’re “bad”); toggle via treat_spike_kneel_as_na\n",
        "      • Non-scrimmage (kickoff/punt/FG/XP/2pt/no_play) -> NaN by default; toggle via drop_non_scrimmage\n",
        "    \"\"\"\n",
        "\n",
        "    # helpers that always return Series aligned to df.index\n",
        "    def col(name, fill=0, dtype=None):\n",
        "        s = df[name] if name in df.columns else pd.Series(fill, index=df.index)\n",
        "        return s.astype(dtype) if dtype is not None else s\n",
        "\n",
        "    posteam = col(\"posteam\", \"\", dtype=\"string\")\n",
        "    defteam = col(\"defteam\", \"\", dtype=\"string\")\n",
        "\n",
        "    # play type handling (nflfastR 'play_type' is lower-case in canonical data; normalize just in case)\n",
        "    play_type = col(\"play_type\", \"\", dtype=\"string\").str.lower()\n",
        "    is_scrimmage = play_type.isin([\"run\", \"pass\"])\n",
        "    if not drop_non_scrimmage:\n",
        "        is_scrimmage = pd.Series(True, index=df.index)\n",
        "\n",
        "    # spikes / kneels\n",
        "    is_spike = col(\"qb_spike\", 0).fillna(0).astype(bool)\n",
        "    is_kneel = col(\"qb_kneel\", 0).fillna(0).astype(bool)\n",
        "    make_na = (is_spike | is_kneel) if treat_spike_kneel_as_na else pd.Series(False, index=df.index)\n",
        "\n",
        "    # penalties\n",
        "    penalty = col(\"penalty\", 0).fillna(0).astype(bool)\n",
        "    penalty_team = col(\"penalty_team\", \"\", dtype=\"string\")\n",
        "    pen_yds = col(\"penalty_yards\", 0).fillna(0)\n",
        "\n",
        "    off_pen = penalty & (penalty_team == posteam)\n",
        "    def_pen = penalty & (penalty_team == defteam)\n",
        "\n",
        "    # turnovers against offense\n",
        "    interception = col(\"interception\", 0).fillna(0).astype(bool)\n",
        "    fumble_lost = col(\"fumble_lost\", 0).fillna(0).astype(bool)\n",
        "    f1 = col(\"fumbled_1_team\", \"\", dtype=\"string\")\n",
        "    f2 = col(\"fumbled_2_team\", \"\", dtype=\"string\")\n",
        "    fumbled_by_off = (f1.eq(posteam) | f2.eq(posteam))\n",
        "    fourth_failed = col(\"fourth_down_failed\", 0).fillna(0).astype(bool)\n",
        "    safety = col(\"safety\", 0).fillna(0).astype(bool)\n",
        "    turnover = interception | (fumble_lost & fumbled_by_off) | fourth_failed | safety\n",
        "\n",
        "    # touchdowns for offense\n",
        "    td_any = col(\"touchdown\", 0).fillna(0).astype(bool)\n",
        "    pass_td = col(\"pass_touchdown\", 0).fillna(0).astype(bool)\n",
        "    rush_td = col(\"rush_touchdown\", 0).fillna(0).astype(bool)\n",
        "    td_team = col(\"td_team\", \"\", dtype=\"string\")\n",
        "    td_off = td_any & ((td_team == posteam) | pass_td | rush_td)\n",
        "\n",
        "    # base share-of-field gained (bounded 0..1)\n",
        "    pre = col(\"yardline_100\").fillna(100).clip(lower=1)  # avoid div/0; treat unknown as far from goal\n",
        "    gain = col(\"yards_gained\").fillna(0)\n",
        "    base_share = (gain.clip(lower=0).divide(pre)).clip(0, 1)\n",
        "\n",
        "    # if defensive penalty, let penalty yards count if larger than recorded gain\n",
        "    pen_share = (pd.Series(np.maximum(gain, pen_yds), index=df.index)\n",
        "                 .divide(pre).clip(0, 1))\n",
        "    y = base_share.where(~def_pen, pen_share).astype(float)\n",
        "    y.name = \"success_outcome\"\n",
        "\n",
        "    # apply overrides (keep as Series to avoid numpy scalar/assignment issues)\n",
        "    y = y.where(~turnover, 0.0)   # turnovers -> 0\n",
        "    y = y.where(~off_pen, 0.0)    # offensive penalty -> 0\n",
        "    y = y.where(~td_off, 1.0)     # offensive TD -> 1\n",
        "\n",
        "    # mask out non-scrimmage and spikes/kneels if desired\n",
        "    y = y.where(is_scrimmage)\n",
        "    y = y.where(~make_na)\n",
        "\n",
        "    # also drop explicit no_play rows if present\n",
        "    no_play = col(\"play_type\", \"\", dtype=\"string\").str.lower().eq(\"no_play\")\n",
        "    y = y.where(~no_play)\n",
        "\n",
        "    return y\n",
        "\n",
        "df[\"success_outcome\"] = compute_success_outcome(df)\n",
        "\n",
        "df[\"success_outcome\"] = (\n",
        "    df[\"yards_gained\"].clip(lower=0, upper=df[\"yardline_100\"])   # no overshoot\n",
        "    .divide(df[\"yardline_100\"].clip(lower=1))                    # avoid /0\n",
        "    .clip(0, 1)                                                  # enforce bounds\n",
        ")\n",
        "\n",
        "s = (\n",
        "    df[\"yards_gained\"].clip(lower=0, upper=df[\"yardline_100\"])\n",
        "    / df[\"yardline_100\"].clip(lower=1)\n",
        ").clip(0, 1)\n",
        "\n",
        "# hard overrides\n",
        "s.loc[(df[\"interception\"]==1) | (df[\"fumble_lost\"]==1)] = 0\n",
        "s.loc[df[\"touchdown\"]==1] = 1\n",
        "s.loc[df[\"fourth_down_failed\"]==1] = 0\n",
        "s.loc[df[\"safety\"]==1] = 0\n",
        "\n",
        "df[\"success_outcome\"] = s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "wBai_LHtvoPr",
        "outputId": "ee815670-0112-42d2-fd20-70225e805e47"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    137851.000000\n",
              "mean          0.124383\n",
              "std           0.223858\n",
              "min           0.000000\n",
              "25%           0.000000\n",
              "50%           0.021739\n",
              "75%           0.142857\n",
              "max           1.000000\n",
              "Name: success_outcome, dtype: float64"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['success_outcome'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps-rKLy7f_kc"
      },
      "source": [
        "**Recode Categorical**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "rEB4oz-w-5Vs"
      },
      "outputs": [],
      "source": [
        "df['under_two_minute_warning'] = (df['half_seconds_remaining'] < 120).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "tfpoEpcssE5H"
      },
      "outputs": [],
      "source": [
        "df[\"offense_drive_number\"] = (\n",
        "    df.sort_values([\"game_id\", \"drive\"])\n",
        "      .groupby([\"game_id\", \"posteam\"])[\"drive\"]\n",
        "      .transform(lambda x: pd.factorize(x)[0] + 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "6M81n3AvqO0F",
        "outputId": "d68485bc-0390-4b8d-9dcb-e179bcfeada9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0    49983\n",
              "2.0    37793\n",
              "3.0    24167\n",
              "4.0    13069\n",
              "Name: down, dtype: int64"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['down'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "                      53347\n",
              "QUICK OUT              7247\n",
              "HITCH/CURL             6550\n",
              "SCREEN                 5930\n",
              "GO                     5100\n",
              "IN/DIG                 3579\n",
              "SLANT                  3567\n",
              "HITCH                  2937\n",
              "DEEP OUT               2770\n",
              "SHALLOW CROSS/DRAG     2629\n",
              "FLAT                   2601\n",
              "POST                   2532\n",
              "OUT                    2528\n",
              "CORNER                 2285\n",
              "CROSS                  2076\n",
              "SWING                  1659\n",
              "IN                     1178\n",
              "ANGLE                   694\n",
              "WHEEL                   626\n",
              "TEXAS/ANGLE             414\n",
              "Name: route, dtype: int64"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['route'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH3wOvI5Nohy",
        "outputId": "0725611f-d02b-4725-9358-e6b07a8e18b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dummy columns created:\n",
            "['pass_location_left', 'pass_location_middle', 'pass_location_right', 'game_half_Half1', 'game_half_Half2', 'game_half_Overtime', 'roof_closed', 'roof_dome', 'roof_open', 'roof_outdoors', 'surface_a_turf', 'surface_astroturf', 'surface_fieldturf', 'surface_grass', 'surface_matrixturf', 'surface_sportturf', 'offense_formation_EMPTY', 'offense_formation_I_FORM', 'offense_formation_JUMBO', 'offense_formation_PISTOL', 'offense_formation_SHOTGUN', 'offense_formation_SINGLEBACK', 'offense_formation_UNDER CENTER', 'offense_formation_WILDCAT', 'route_ANGLE', 'route_CORNER', 'route_CROSS', 'route_DEEP OUT', 'route_FLAT', 'route_GO', 'route_HITCH', 'route_HITCH/CURL', 'route_IN', 'route_IN/DIG', 'route_OUT', 'route_POST', 'route_QUICK OUT', 'route_SCREEN', 'route_SHALLOW CROSS/DRAG', 'route_SLANT', 'route_SWING', 'route_TEXAS/ANGLE', 'route_WHEEL', 'defense_man_zone_type_MAN_COVERAGE', 'defense_man_zone_type_ZONE_COVERAGE', 'defense_coverage_type_2_MAN', 'defense_coverage_type_BLOWN', 'defense_coverage_type_COMBO', 'defense_coverage_type_COVER_0', 'defense_coverage_type_COVER_1', 'defense_coverage_type_COVER_2', 'defense_coverage_type_COVER_3', 'defense_coverage_type_COVER_4', 'defense_coverage_type_COVER_6', 'defense_coverage_type_COVER_9', 'defense_coverage_type_PREVENT']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/bc/_mksvg913475jdkwl9xxysdc0000gn/T/ipykernel_38080/935341123.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[\"season_postseason\"] = (df[\"season_type\"] == \"POST\").astype(int)\n"
          ]
        }
      ],
      "source": [
        "# Make dummies\n",
        "cols_to_dummy = [\"pass_location\", \"game_half\",'roof','surface','offense_formation','route','defense_man_zone_type','defense_coverage_type']\n",
        "\n",
        "all_dummy_cols = []\n",
        "\n",
        "for col in cols_to_dummy:\n",
        "    # make dummies\n",
        "    df[col] = df[col].replace(\"\", pd.NA)\n",
        "    dummies = pd.get_dummies(df[col], prefix=col, drop_first=False)\n",
        "\n",
        "    # record the new columns\n",
        "    all_dummy_cols.extend(dummies.columns.tolist())\n",
        "\n",
        "    # replace the original column with the dummies\n",
        "    df = pd.concat([df.drop(columns=[col]), dummies], axis=1)\n",
        "\n",
        "# print everything in clean list format\n",
        "print(\"\\nDummy columns created:\")\n",
        "print(all_dummy_cols)\n",
        "\n",
        "# Recode Hometeam\n",
        "df[\"posteam_home\"] = (df[\"posteam_type\"] == \"home\").astype(int)\n",
        "\n",
        "#\n",
        "df[\"season_postseason\"] = (df[\"season_type\"] == \"POST\").astype(int)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpo7XeI0ygsL"
      },
      "source": [
        "**Time Columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHbIC1yJyivt",
        "outputId": "d046f03a-3c8e-4482-d41a-d8f348ed24ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time feature columns (copy-paste):\n",
            "[\n",
            "  'tod_utc',\n",
            "  'tod_tz_name',\n",
            "  'tod_local',\n",
            "  'tod_hour',\n",
            "  'tod_minute',\n",
            "  'tod_weekday',\n",
            "  'tod_is_weekend',\n",
            "  'tod_is_night',\n",
            "  'tod_sec_midnight',\n",
            "  'tod_sin_time',\n",
            "  'tod_cos_time',\n",
            "  'tod_month',\n",
            "  'tod_unix'\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "NFL_TEAM_TZ = {\n",
        "    \"BUF\":\"America/New_York\",\"MIA\":\"America/New_York\",\"NE\":\"America/New_York\",\"NYJ\":\"America/New_York\",\n",
        "    \"BAL\":\"America/New_York\",\"CIN\":\"America/New_York\",\"CLE\":\"America/New_York\",\"PIT\":\"America/New_York\",\n",
        "    \"HOU\":\"America/Chicago\",\"IND\":\"America/Indiana/Indianapolis\",\"JAX\":\"America/New_York\",\"TEN\":\"America/Chicago\",\n",
        "    \"KC\":\"America/Chicago\",\"LV\":\"America/Los_Angeles\",\"LAC\":\"America/Los_Angeles\",\"DEN\":\"America/Denver\",\n",
        "    \"DAL\":\"America/Chicago\",\"NYG\":\"America/New_York\",\"PHI\":\"America/New_York\",\"WAS\":\"America/New_York\",\"WSH\":\"America/New_York\",\n",
        "    \"CHI\":\"America/Chicago\",\"DET\":\"America/Detroit\",\"GB\":\"America/Chicago\",\"MIN\":\"America/Chicago\",\n",
        "    \"ATL\":\"America/New_York\",\"CAR\":\"America/New_York\",\"NO\":\"America/Chicago\",\"TB\":\"America/New_York\",\n",
        "    \"ARI\":\"America/Phoenix\",\"SF\":\"America/Los_Angeles\",\"SEA\":\"America/Los_Angeles\",\n",
        "    \"LA\":\"America/Los_Angeles\",\"LAR\":\"America/Los_Angeles\"\n",
        "}\n",
        "\n",
        "def add_time_features(\n",
        "    df: pd.DataFrame,\n",
        "    time_col: str = \"time_of_day\",\n",
        "    tz_key_col: str = \"home_team\",\n",
        "    prefix: str = \"tod_\",\n",
        "    print_cols: bool = True\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Parse ISO 8601 timestamps -> UTC; convert to local (by tz_key_col) -> tz-naive;\n",
        "    derive hour/weekday/flags + cyclical features. Avoids .dt errors.\n",
        "    \"\"\"\n",
        "    out = df.copy()\n",
        "\n",
        "    # 1) Parse to UTC (handles 'Z' and offsets; invalid -> NaT)\n",
        "    ts_utc = pd.to_datetime(out.get(time_col), errors=\"coerce\", utc=True)\n",
        "    out[f\"{prefix}utc\"] = ts_utc\n",
        "\n",
        "    # 2) Map to timezone names; fallback to UTC\n",
        "    tz_series = out.get(tz_key_col).astype(\"string\").map(NFL_TEAM_TZ).fillna(\"UTC\")\n",
        "    out[f\"{prefix}tz_name\"] = tz_series\n",
        "\n",
        "    # 3) Convert per-timezone group, then DROP tz (tz-naive) to keep dtype uniform\n",
        "    local_naive = pd.Series(pd.NaT, index=out.index, dtype=\"datetime64[ns]\")\n",
        "    for tz_name, idx in out.groupby(f\"{prefix}tz_name\").groups.items():\n",
        "        if len(idx) == 0:\n",
        "            continue\n",
        "        # convert UTC -> local tz, then drop tz (tz_localize(None))\n",
        "        loc = ts_utc.loc[idx].dt.tz_convert(tz_name).dt.tz_localize(None)\n",
        "        local_naive.loc[idx] = loc.values\n",
        "    out[f\"{prefix}local\"] = local_naive  # dtype: datetime64[ns] (uniform -> .dt works)\n",
        "\n",
        "    # 4) Derive features from local time (safe .dt access)\n",
        "    ts_loc = out[f\"{prefix}local\"]\n",
        "\n",
        "    out[f\"{prefix}hour\"] = ts_loc.dt.hour\n",
        "    out[f\"{prefix}minute\"] = ts_loc.dt.minute\n",
        "    out[f\"{prefix}weekday\"] = ts_loc.dt.weekday  # Mon=0..Sun=6\n",
        "    out[f\"{prefix}is_weekend\"] = out[f\"{prefix}weekday\"].isin([5, 6]).astype(\"int8\")\n",
        "    out[f\"{prefix}is_night\"] = ((out[f\"{prefix}hour\"] >= 20) | (out[f\"{prefix}hour\"] <= 6)).astype(\"int8\")\n",
        "\n",
        "    # Seconds since local midnight (float; NaN if ts_loc is NaT)\n",
        "    sec_midnight = (\n",
        "        ts_loc.dt.hour.fillna(0)*3600\n",
        "        + ts_loc.dt.minute.fillna(0)*60\n",
        "        + ts_loc.dt.second.fillna(0)\n",
        "    ).astype(float)\n",
        "    out[f\"{prefix}sec_midnight\"] = sec_midnight\n",
        "\n",
        "    # Cyclical encodings (time-of-day)\n",
        "    angle = 2 * np.pi * (sec_midnight / 86400.0)\n",
        "    out[f\"{prefix}sin_time\"] = np.sin(angle)\n",
        "    out[f\"{prefix}cos_time\"] = np.cos(angle)\n",
        "\n",
        "    # Optional seasonality & epoch seconds\n",
        "    out[f\"{prefix}month\"] = ts_loc.dt.month\n",
        "    # epoch seconds as float (handles NaT -> NaN cleanly)\n",
        "    out[f\"{prefix}unix\"] = (ts_loc - pd.Timestamp(\"1970-01-01\")).dt.total_seconds()\n",
        "\n",
        "    new_cols = [\n",
        "        f\"{prefix}utc\", f\"{prefix}tz_name\", f\"{prefix}local\",\n",
        "        f\"{prefix}hour\", f\"{prefix}minute\", f\"{prefix}weekday\",\n",
        "        f\"{prefix}is_weekend\", f\"{prefix}is_night\",\n",
        "        f\"{prefix}sec_midnight\", f\"{prefix}sin_time\", f\"{prefix}cos_time\",\n",
        "        f\"{prefix}month\", f\"{prefix}unix\"\n",
        "    ]\n",
        "    if print_cols:\n",
        "        print(\"Time feature columns (copy-paste):\\n[\\n  '\" + \"',\\n  '\".join(new_cols) + \"'\\n]\")\n",
        "\n",
        "    return out\n",
        "df = add_time_features(df, time_col=\"time_of_day\", tz_key_col=\"home_team\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2F1slMF0GpS"
      },
      "source": [
        "**Clean Weather**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "MQbaJ75O0IVR"
      },
      "outputs": [],
      "source": [
        "def add_weather_flags(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    w = df[\"weather\"].fillna(\"\").str.lower()\n",
        "    df[\"is_precip\"] = w.str.contains(\"rain|snow|sleet|shower|wet\").astype(int)\n",
        "    df[\"is_windy\"]  = (w.str.contains(\"wind\") | (df[\"wind\"].fillna(0) > 15)).astype(int)\n",
        "    df[\"is_clear\"]  = ((w.str.contains(\"clear|sun\")) & (df[\"is_precip\"] == 0)).astype(int)\n",
        "    return df\n",
        "df = add_weather_flags(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23F4DZwM1yWM"
      },
      "source": [
        "**Clean Air Yards**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "VJ7AO4n01zqB"
      },
      "outputs": [],
      "source": [
        "def add_airyards_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "\n",
        "    # text field for fallback matching\n",
        "    desc = out.get(\"desc\", pd.Series(\"\", index=out.index)).fillna(\"\").str.lower()\n",
        "\n",
        "    # basic play-type flags\n",
        "    is_pass = out.get(\"pass_attempt\", 0).fillna(0).astype(bool)\n",
        "    is_spike = out.get(\"qb_spike\", 0).fillna(0).astype(bool)\n",
        "\n",
        "    # heuristic throwaway: incomplete pass with no receiver + throwaway language\n",
        "    no_receiver = out.get(\"receiver_player_id\", pd.Series(np.nan, index=out.index)).isna()\n",
        "    incomplete = out.get(\"complete_pass\", 0).fillna(0).astype(bool) == False\n",
        "    is_throwaway = is_pass & incomplete & no_receiver & (\n",
        "        desc.str.contains(\"throwaway|thrown away|threw it away|out of bounds\")\n",
        "    )\n",
        "\n",
        "    # Some feeds also mark \"batted\" or \"spiked\"—exclude those from 'intended depth'\n",
        "    is_batted = desc.str.contains(\"batted|tipped\") & is_pass & incomplete\n",
        "\n",
        "    # single control flag\n",
        "    out[\"is_throwaway_or_spike\"] = (is_throwaway | is_spike).astype(int)\n",
        "\n",
        "    # Keep true air_yards for real targets; null them for throwaways/spikes\n",
        "    air = out.get(\"air_yards\", pd.Series(np.nan, index=out.index))\n",
        "    out[\"air_yards_clean\"] = air.where(~out[\"is_throwaway_or_spike\"].astype(bool))\n",
        "\n",
        "    # For tree models: provide a numeric-imputed version + the flag\n",
        "    # (RF can't take NaN; the flag preserves information)\n",
        "    out[\"air_yards_for_model\"] = out[\"air_yards_clean\"].fillna(0)\n",
        "\n",
        "    # Optional: cap extreme tails but KEEP negatives (screens)\n",
        "    out[\"air_yards_for_model\"] = out[\"air_yards_for_model\"].clip(lower=-15, upper=60)\n",
        "\n",
        "    # Optional extras the QB would know:\n",
        "    out[\"is_batted_pass\"] = is_batted.astype(int)\n",
        "    out[\"is_screen_like\"]  = (air <= -1).fillna(False).astype(int)  # behind LOS\n",
        "\n",
        "    return out\n",
        "df = add_airyards_clean(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnerZe5VhShE"
      },
      "source": [
        "**Filter Only Plays We Care to Observe**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "CD8MfiqmRUbX",
        "outputId": "6efc1669-a440-4f0e-e439-25c1052ae6dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         3.370\n",
              "1         1.768\n",
              "4         1.802\n",
              "7         2.068\n",
              "9         2.536\n",
              "          ...  \n",
              "148549    7.900\n",
              "148552    3.500\n",
              "148557    1.600\n",
              "148559    2.500\n",
              "148575    2.100\n",
              "Name: time_to_throw, Length: 55115, dtype: float32"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Filter the DataFrame to find observations where yards_gained is greater than yrds_to_ez\n",
        "weird_plays = df[df['yards_gained'] > df['yardline_100']]\n",
        "# Drop the weird_plays from the original DataFrame\n",
        "df = df.drop(weird_plays.index)\n",
        "df = df.query('special_teams_play==0')\n",
        "df = df.query(\"play_type_nfl == 'PASS'\")\n",
        "df['time_to_throw'].dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.sort_values(['game_id', 'drive', 'play_id'])\n",
        "df['drive_play_index'] = df.groupby(['game_id', 'drive']).cumcount() + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "kshvNiEcpe55"
      },
      "outputs": [],
      "source": [
        "X = ['posteam_home',\n",
        "     'under_two_minute_warning',\n",
        "     'season_postseason',\n",
        "     'week',\n",
        "     'quarter_seconds_remaining',\n",
        "     'half_seconds_remaining',\n",
        "     'game_seconds_remaining',\n",
        "     'offense_drive_number',\n",
        "     'qtr',\n",
        "     'down',\n",
        "     'yards_after_catch',\n",
        "     'posteam_timeouts_remaining',\n",
        "     'defteam_timeouts_remaining',\n",
        "     'posteam_score',\n",
        "     'defteam_score',\n",
        "     'score_differential',\n",
        "     'air_yards_for_model',\n",
        "     'is_throwaway_or_spike',\n",
        "     'is_batted_pass',\n",
        "     'is_screen_like',\n",
        "\n",
        "    #In-Game Counts\n",
        "     'punt_blocked_so_far', 'first_down_rush_so_far', 'first_down_pass_so_far', 'first_down_penalty_so_far', 'third_down_converted_so_far', 'third_down_failed_so_far', 'fourth_down_converted_so_far', 'fourth_down_failed_so_far', 'incomplete_pass_so_far', 'touchback_so_far', 'punt_inside_twenty_so_far', 'punt_in_endzone_so_far', 'punt_out_of_bounds_so_far', 'punt_downed_so_far', 'punt_fair_catch_so_far', 'interception_so_far', 'fumble_forced_so_far', 'fumble_not_forced_so_far', 'fumble_out_of_bounds_so_far', 'fumble_lost_so_far', 'fumble_so_far', 'solo_tackle_so_far', 'assist_tackle_so_far', 'tackled_for_loss_so_far', 'qb_hit_so_far', 'sack_so_far', 'penalty_so_far', 'touchdown_so_far', 'pass_touchdown_so_far', 'rush_touchdown_so_far', 'return_touchdown_so_far', 'extra_point_attempt_so_far', 'two_point_attempt_so_far', 'field_goal_attempt_so_far', 'kickoff_attempt_so_far', 'punt_attempt_so_far', 'rush_attempt_so_far', 'pass_attempt_so_far', 'complete_pass_so_far', 'lateral_reception_so_far', 'lateral_rush_so_far', 'safety_so_far', 'return_yards_so_far', 'penalty_yards_so_far',\n",
        "\n",
        "    #In-Game Rates\n",
        "     'completion_rate_so_far',\n",
        "     'pass_rate_so_far',\n",
        "\n",
        "     'season',\n",
        "     'tod_hour',\n",
        "     'tod_minute',\n",
        "     'tod_weekday',\n",
        "     'tod_is_weekend',\n",
        "     'tod_is_night',\n",
        "     'tod_sec_midnight',\n",
        "     'tod_sin_time',\n",
        "     'tod_cos_time',\n",
        "     'tod_month',\n",
        "     'tod_unix',\n",
        "     'is_precip',\n",
        "     'is_windy',\n",
        "     'is_clear',\n",
        "     'drive_play_index',\n",
        "     'temp',\n",
        "     'wind',\n",
        "     'defenders_in_box',\n",
        "     'number_of_pass_rushers',\n",
        "\n",
        "     #Things of Interest\n",
        "     'time_to_throw',\n",
        "     'was_pressure',\n",
        "\n",
        "     #Dummies\n",
        "     'pass_location_left', 'pass_location_middle', 'pass_location_right', 'game_half_Half1', 'game_half_Half2', 'game_half_Overtime', 'roof_closed', 'roof_dome', 'roof_open', 'roof_outdoors', 'surface_a_turf', 'surface_astroturf', 'surface_fieldturf', 'surface_grass', 'surface_matrixturf', 'surface_sportturf', 'offense_formation_EMPTY', 'offense_formation_I_FORM', 'offense_formation_JUMBO', 'offense_formation_PISTOL', 'offense_formation_SHOTGUN', 'offense_formation_SINGLEBACK', 'offense_formation_UNDER CENTER', 'offense_formation_WILDCAT', 'route_ANGLE', 'route_CORNER', 'route_CROSS', 'route_DEEP OUT', 'route_FLAT', 'route_GO', 'route_HITCH', 'route_HITCH/CURL', 'route_IN', 'route_IN/DIG', 'route_OUT', 'route_POST', 'route_QUICK OUT', 'route_SCREEN', 'route_SHALLOW CROSS/DRAG', 'route_SLANT', 'route_SWING', 'route_TEXAS/ANGLE', 'route_WHEEL', 'defense_man_zone_type_MAN_COVERAGE', 'defense_man_zone_type_ZONE_COVERAGE', 'defense_coverage_type_2_MAN', 'defense_coverage_type_BLOWN', 'defense_coverage_type_COMBO', 'defense_coverage_type_COVER_0', 'defense_coverage_type_COVER_1', 'defense_coverage_type_COVER_2', 'defense_coverage_type_COVER_3', 'defense_coverage_type_COVER_4', 'defense_coverage_type_COVER_6', 'defense_coverage_type_COVER_9', 'defense_coverage_type_PREVENT']\n",
        "\n",
        "y = 'success_outcome'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Huqar7mM09yz"
      },
      "outputs": [],
      "source": [
        "# Drop any rows missing the target\n",
        "df = df.dropna(subset=[\"success_outcome\"])\n",
        "\n",
        "# Ensure all X columns exist and are numeric\n",
        "for col in X:\n",
        "    if col not in df.columns:\n",
        "        print(f\"Missing feature: {col}\")\n",
        "X_data = df[X].apply(pd.to_numeric, errors=\"coerce\").fillna(0).astype(float)\n",
        "y_data = df[\"success_outcome\"].astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "mcpUxKMwkaCk"
      },
      "outputs": [],
      "source": [
        "df.to_csv('cleaned_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "NlWOe5HSYDrT"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# OUTDIR = \"/content/drive/MyDrive/qb_success_results\"  # change if you like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "mjp6g7Bw1CvM"
      },
      "outputs": [],
      "source": [
        "# import os, json, joblib, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "# from pathlib import Path\n",
        "\n",
        "# from sklearn.model_selection import GroupKFold, GroupShuffleSplit, RandomizedSearchCV\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# from sklearn.preprocessing import FunctionTransformer\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from sklearn.feature_selection import SelectFromModel\n",
        "# from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
        "# from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, make_scorer\n",
        "# from sklearn.inspection import permutation_importance\n",
        "\n",
        "# import shap\n",
        "\n",
        "# # ---------------------------\n",
        "# # Utils\n",
        "# # ---------------------------\n",
        "# def _ensure_numeric_frame(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
        "#     cols_existing = [c for c in cols if c in df.columns]\n",
        "#     if len(cols_existing) < len(cols):\n",
        "#         missing = sorted(set(cols) - set(cols_existing))\n",
        "#         print(f\"[warn] missing features dropped: {missing[:10]}{' ...' if len(missing)>10 else ''}\")\n",
        "#     X = df[cols_existing].copy()\n",
        "#     X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
        "#     return X\n",
        "\n",
        "# def _train_val_split_by_game(df: pd.DataFrame, test_size=0.2, random_state=42):\n",
        "#     splitter = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
        "#     idx_tr, idx_val = next(splitter.split(df, groups=df[\"game_id\"]))\n",
        "#     return idx_tr, idx_val\n",
        "\n",
        "# def _mkdir(path):\n",
        "#     Path(path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# # ---------------------------\n",
        "# # Main builder (overnight-ready)\n",
        "# # ---------------------------\n",
        "# def build_and_train_qb_success_model(\n",
        "#     df: pd.DataFrame,\n",
        "#     feature_cols: list[str],\n",
        "#     target_col: str = \"success_outcome\",\n",
        "#     group_col: str = \"game_id\",\n",
        "#     outdir: str = \"./qb_success_results\",\n",
        "#     random_state: int = 42,\n",
        "#     n_iter_search: int = 40,\n",
        "#     shap_sample: int = 5000,      # subsample for SHAP to keep runtime reasonable\n",
        "#     perm_repeats: int = 10,       # permutation importance repeats\n",
        "#     verbose: int = 3,             # more logs from RandomizedSearchCV\n",
        "# ):\n",
        "#     _mkdir(outdir)\n",
        "\n",
        "#     # ---------------- Sanity\n",
        "#     if target_col not in df.columns:\n",
        "#         raise KeyError(f\"Target column '{target_col}' not found\")\n",
        "#     df = df.dropna(subset=[target_col]).copy()\n",
        "\n",
        "#     # ---------------- Data\n",
        "#     X_raw = _ensure_numeric_frame(df, feature_cols)\n",
        "#     y = pd.to_numeric(df[target_col], errors=\"coerce\").astype(float)\n",
        "#     groups = df[group_col]\n",
        "\n",
        "#     # ---------------- Pipeline\n",
        "#     num_selector = FunctionTransformer(lambda X: X, feature_names_out=\"one-to-one\")\n",
        "#     pre = ColumnTransformer(\n",
        "#         transformers=[(\"num\", num_selector, X_raw.columns)],\n",
        "#         remainder=\"drop\",\n",
        "#         sparse_threshold=0.0,\n",
        "#         verbose_feature_names_out=False,\n",
        "#     )\n",
        "\n",
        "#     imputer = SimpleImputer(strategy=\"median\", add_indicator=True)\n",
        "\n",
        "#     selector = SelectFromModel(\n",
        "#         RandomForestRegressor(\n",
        "#             n_estimators=300, max_depth=None, n_jobs=-1, random_state=random_state\n",
        "#         ),\n",
        "#         threshold=\"median\",\n",
        "#         prefit=False\n",
        "#     )\n",
        "\n",
        "#     hgb = HistGradientBoostingRegressor(\n",
        "#         random_state=random_state, early_stopping=True, validation_fraction=0.1, max_iter=300\n",
        "#     )\n",
        "\n",
        "#     pipe = Pipeline([\n",
        "#         (\"pre\", pre),\n",
        "#         (\"imputer\", imputer),\n",
        "#         (\"selector\", selector),\n",
        "#         (\"model\", hgb),\n",
        "#     ])\n",
        "\n",
        "#     # ---------------- Search space\n",
        "#     param_dist = {\n",
        "#         \"selector__threshold\": [\"median\",\"0.75*median\",\"1.0*median\",\"1.25*median\",\"2*median\"],\n",
        "#         \"model__learning_rate\":  np.geomspace(0.01, 0.2, 10),\n",
        "#         \"model__max_depth\":      [None, 4, 6, 8, 10],\n",
        "#         \"model__max_leaf_nodes\": [None, 31, 63, 127],\n",
        "#         \"model__min_samples_leaf\": [10, 20, 50, 100],\n",
        "#         \"model__l2_regularization\": np.geomspace(1e-6, 1e-1, 8),\n",
        "#     }\n",
        "\n",
        "#     gkf = GroupKFold(n_splits=5)\n",
        "#     neg_mae = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "\n",
        "#     search = RandomizedSearchCV(\n",
        "#         estimator=pipe,\n",
        "#         param_distributions=param_dist,\n",
        "#         n_iter=n_iter_search,\n",
        "#         scoring={\"neg_mae\": neg_mae, \"r2\": \"r2\"},\n",
        "#         refit=\"neg_mae\",\n",
        "#         cv=gkf.split(X_raw, y, groups),\n",
        "#         n_jobs=-1,\n",
        "#         verbose=verbose,\n",
        "#         random_state=random_state,\n",
        "#         return_train_score=True,\n",
        "#     )\n",
        "\n",
        "#     print(\"[info] starting RandomizedSearchCV...\")\n",
        "#     search.fit(X_raw, y)\n",
        "#     print(\"[info] search complete.\")\n",
        "\n",
        "#     # save cv results\n",
        "#     cv_df = pd.DataFrame(search.cv_results_)\n",
        "#     cv_df.to_csv(os.path.join(outdir, \"cv_results.csv\"), index=False)\n",
        "#     joblib.dump(search, os.path.join(outdir, \"search.pkl\"))\n",
        "\n",
        "#     # ---------------- Hold-out eval\n",
        "#     idx_tr, idx_val = _train_val_split_by_game(df, test_size=0.2, random_state=random_state)\n",
        "#     X_tr, X_val = X_raw.iloc[idx_tr], X_raw.iloc[idx_val]\n",
        "#     y_tr, y_val = y.iloc[idx_tr], y.iloc[idx_val]\n",
        "\n",
        "#     best_pipe = search.best_estimator_\n",
        "#     best_pipe.fit(X_tr, y_tr)\n",
        "\n",
        "#     y_pred = best_pipe.predict(X_val)\n",
        "\n",
        "#     r2  = r2_score(y_val, y_pred)\n",
        "#     mae = mean_absolute_error(y_val, y_pred)\n",
        "#     mse = mean_squared_error(y_val, y_pred)\n",
        "#     rmse = np.sqrt(mse)\n",
        "\n",
        "#     metrics = {\n",
        "#         \"val_r2\": r2,\n",
        "#         \"val_mae\": mae,\n",
        "#         \"val_mse\": mse,\n",
        "#         \"val_rmse\": rmse,\n",
        "#         \"cv_best_neg_mae\": float(search.best_score_),\n",
        "#         \"cv_best_params\": search.best_params_,\n",
        "#         \"n_features_input\": int(X_raw.shape[1]),\n",
        "#     }\n",
        "#     print(\"[metrics]\", json.dumps(metrics, indent=2))\n",
        "#     with open(os.path.join(outdir, \"metrics.json\"), \"w\") as f:\n",
        "#         json.dump(metrics, f, indent=2)\n",
        "\n",
        "#     # save model\n",
        "#     joblib.dump(best_pipe, os.path.join(outdir, \"best_pipeline.pkl\"))\n",
        "\n",
        "#     # ---------------- Permutation importance\n",
        "#     print(\"[info] computing permutation importance ...\")\n",
        "#     perm = permutation_importance(best_pipe, X_val, y_val,\n",
        "#                                   n_repeats=perm_repeats, random_state=random_state, n_jobs=-1)\n",
        "#     pre_names = best_pipe.named_steps[\"pre\"].get_feature_names_out()\n",
        "#     n_orig = len(pre_names)\n",
        "#     indicator_names = [f\"{n}_missing\" for n in pre_names]\n",
        "#     imputed_feature_names = list(pre_names) + indicator_names\n",
        "#     support_mask = best_pipe.named_steps[\"selector\"].get_support()\n",
        "#     selected_feature_names = [f for f, keep in zip(imputed_feature_names, support_mask) if keep]\n",
        "\n",
        "#     pi = pd.Series(perm.importances_mean, index=selected_feature_names).sort_values(ascending=False)\n",
        "#     pi.to_csv(os.path.join(outdir, \"permutation_importance.csv\"))\n",
        "#     print(\"[top PI]\\n\", pi.head(25))\n",
        "\n",
        "#     # ---------------- Residual diagnostics\n",
        "#     resid = y_val - y_pred\n",
        "#     diag = pd.DataFrame({\"y_true\": y_val, \"y_pred\": y_pred, \"residual\": resid}, index=X_val.index)\n",
        "#     diag.to_csv(os.path.join(outdir, \"residuals_val.csv\"))\n",
        "\n",
        "#     plt.figure()\n",
        "#     plt.scatter(y_pred, resid, s=4, alpha=0.3)\n",
        "#     plt.axhline(0, color=\"black\", linewidth=1)\n",
        "#     plt.xlabel(\"Predicted success_outcome\"); plt.ylabel(\"Residual\")\n",
        "#     plt.title(\"Residuals vs Predicted\")\n",
        "#     plt.tight_layout()\n",
        "#     plt.savefig(os.path.join(outdir, \"residuals_vs_pred.png\"), dpi=300)\n",
        "#     plt.close()\n",
        "\n",
        "#     # ---------------- SHAP (summaries + plots)\n",
        "#     # transform X_val through pre+imputer+selector to align with model inputs\n",
        "#     print(\"[info] computing SHAP on validation subsample ...\")\n",
        "#     # pipeline slicing: everything except last step (model)\n",
        "#     pre_to_sel = Pipeline(best_pipe.steps[:-1])\n",
        "#     Xt_val = pre_to_sel.transform(X_val)\n",
        "#     # align names to selected features\n",
        "#     Xt_feature_names = selected_feature_names\n",
        "\n",
        "#     # subsample for speed\n",
        "#     if shap_sample and Xt_val.shape[0] > shap_sample:\n",
        "#         samp_idx = np.random.RandomState(random_state).choice(Xt_val.shape[0], shap_sample, replace=False)\n",
        "#         Xt_val_shap = Xt_val[samp_idx]\n",
        "#     else:\n",
        "#         Xt_val_shap = Xt_val\n",
        "\n",
        "#     # Tree-based explainer works with HGB\n",
        "#     model_core = best_pipe.named_steps[\"model\"]\n",
        "#     explainer = shap.Explainer(model_core, feature_names=Xt_feature_names)\n",
        "#     shap_values = explainer(Xt_val_shap)\n",
        "\n",
        "#     # save raw shap values summary (mean |shap|)\n",
        "#     mean_abs_shap = np.abs(shap_values.values).mean(axis=0)\n",
        "#     shap_summary = pd.Series(mean_abs_shap, index=Xt_feature_names).sort_values(ascending=False)\n",
        "#     shap_summary.to_csv(os.path.join(outdir, \"shap_mean_abs.csv\"))\n",
        "\n",
        "#     # plots\n",
        "#     shap.plots.bar(shap_values, max_display=30, show=False)\n",
        "#     plt.tight_layout()\n",
        "#     plt.savefig(os.path.join(outdir, \"shap_bar_top30.png\"), dpi=300)\n",
        "#     plt.close()\n",
        "\n",
        "#     shap.plots.beeswarm(shap_values, max_display=30, show=False)\n",
        "#     plt.tight_layout()\n",
        "#     plt.savefig(os.path.join(outdir, \"shap_beeswarm_top30.png\"), dpi=300)\n",
        "#     plt.close()\n",
        "\n",
        "#     # store a small table for notebook printing\n",
        "#     top20 = shap_summary.head(20)\n",
        "#     top20.to_csv(os.path.join(outdir, \"shap_top20.csv\"))\n",
        "\n",
        "#     # ---------------- Return summary\n",
        "#     report = {\n",
        "#         \"metrics\": metrics,\n",
        "#         \"permutation_importance_head\": pi.head(25),\n",
        "#         \"selected_features\": selected_feature_names,\n",
        "#         \"cv_results_file\": os.path.join(outdir, \"cv_results.csv\"),\n",
        "#         \"model_file\": os.path.join(outdir, \"best_pipeline.pkl\"),\n",
        "#         \"shap_bar\": os.path.join(outdir, \"shap_bar_top30.png\"),\n",
        "#         \"shap_beeswarm\": os.path.join(outdir, \"shap_beeswarm_top30.png\"),\n",
        "#         \"residuals_plot\": os.path.join(outdir, \"residuals_vs_pred.png\"),\n",
        "#     }\n",
        "#     return search, best_pipe, report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "Lupr99Q94G5t",
        "outputId": "9fabb095-e8cf-4ac1-9dda-a2e7bc831587"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
